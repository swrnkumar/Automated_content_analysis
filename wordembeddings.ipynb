{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with word embeddings\n",
    "Big Data and Automated Content Analysis\n",
    "\n",
    "Damian Trilling\n",
    "\n",
    "\n",
    "This notebook shows you some first steps of how to work with word embeddings in Gensim. \n",
    "\n",
    "\n",
    "**NB  Some of these things may take a lot of memory and/or computing power. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings\n",
    "Training word embedings with gensim is very simple (see example code below - even though you wildo l need to specift some extra options). However, you need a massive dataset for that (think of millions of documents). We're therefore not going to do this in class.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec()\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained models\n",
    "We can use pre-trained embeddings instead. You can either download them yourself and then read them from a file (which you probably want to do if you want, for instance, use our Dutch embeddings that we talked about). \n",
    "But we can also use some embeddings that come with gensim and that gensim downloads for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# alternative that is 16 times as big:\n",
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word similarities\n",
    "Let's play around with word similarieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8798074722290039),\n",
       " ('rabbit', 0.7424426674842834),\n",
       " ('cats', 0.7323004007339478),\n",
       " ('monkey', 0.7288709878921509),\n",
       " ('pet', 0.7190139889717102),\n",
       " ('dogs', 0.7163872718811035),\n",
       " ('mouse', 0.6915250420570374),\n",
       " ('puppy', 0.6800068020820618),\n",
       " ('rat', 0.6641027331352234),\n",
       " ('spider', 0.6501135230064392)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is almost the same as a dog.\n",
      "A dog is almost the same as a cat.\n",
      "A horse is almost the same as a horses.\n",
      "A goldfish is almost the same as a crackers.\n",
      "A lion is almost the same as a dragon.\n"
     ]
    }
   ],
   "source": [
    "animals = ['cat', 'dog', 'horse', 'goldfish', 'lion']\n",
    "for animal in animals:\n",
    "    print(\"A {} is almost the same as a {}.\".format(animal, model.most_similar(animal)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.closer_than('man','boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20851284265518188\n",
      "0.1676505208015442\n"
     ]
    }
   ],
   "source": [
    "print(model.distance('man','boy'))\n",
    "print(model.distance('man','woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as we discussed in the lecture, we can literally calculate with the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755735874176025),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534753799438),\n",
       " ('prince', 0.6517034769058228),\n",
       " ('elizabeth', 0.6464517712593079),\n",
       " ('mother', 0.6311717629432678),\n",
       " ('emperor', 0.6106470823287964),\n",
       " ('wife', 0.6098655462265015)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8551837205886841),\n",
       " ('queen', 0.783441424369812),\n",
       " ('monarch', 0.6933802366256714),\n",
       " ('throne', 0.6833109855651855),\n",
       " ('daughter', 0.680908203125),\n",
       " ('prince', 0.6713142395019531),\n",
       " ('princess', 0.664408266544342),\n",
       " ('mother', 0.6579325199127197),\n",
       " ('elizabeth', 0.6563301086425781),\n",
       " ('father', 0.6392419338226318)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can do the same by hand, but would need to do some manual cleanup afterwards\n",
    "# (e.g., removing 'king' itself from the results)\n",
    "model.similar_by_vector(model.get_vector('king') - model.get_vector('man') + model.get_vector('woman') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY YOURSELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings in supervised machine learning \n",
    "\n",
    "We need to `sudo pip3 install embeddingvectorizer` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import embeddingvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 12500 positive reviews\n",
      "Added 12500 negative reviews\n",
      "Added 12500 positive reviews\n",
      "Added 12500 negative reviews\n"
     ]
    }
   ],
   "source": [
    "reviews=[]\n",
    "test=[]\n",
    "\n",
    "for file in glob (\"/home/damian/Downloads/aclImdb/train/pos/*.txt\"):\n",
    "    with open(file) as fi:\n",
    "        reviews.append((fi.read(),\"1\"))\n",
    "nopostr=len(reviews)\n",
    "print (\"Added\",nopostr,\"positive reviews\")  \n",
    "\n",
    "for file in glob (\"/home/damian/Downloads/aclImdb/train/neg/*.txt\"):\n",
    "    with open(file) as fi:\n",
    "        reviews.append((fi.read(),\"-1\"))\n",
    "nonegtr=len(reviews)-nopostr\n",
    "print (\"Added\",nonegtr,\"negative reviews\")  \n",
    "\n",
    "for file in glob (\"/home/damian/Downloads/aclImdb/test/pos/*.txt\"):\n",
    "    with open(file) as fi:\n",
    "        test.append((fi.read(),\"1\"))\n",
    "noposte=len(test)\n",
    "print (\"Added\",noposte,\"positive reviews\")  \n",
    "\n",
    "for file in glob (\"/home/damian/Downloads/aclImdb/test/neg/*.txt\"):\n",
    "    with open(file) as fi:\n",
    "        test.append((fi.read(),\"-1\"))\n",
    "nonegte=len(test)-noposte\n",
    "print (\"Added\",nonegte,\"negative reviews\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at our old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "0.8598565139409751\n",
      "Recall:\n",
      "0.84376\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                    ('svm', \n",
    "                     SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit([e[0] for e in reviews], [e[1] for e in reviews])\n",
    "predictions = mypipe.predict([e[0] for e in test])\n",
    "\n",
    "print('Precision:')\n",
    "print(metrics.precision_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))\n",
    "print('Recall:')\n",
    "print(metrics.recall_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's compare with the new one.\n",
    "First, we need to convert our word embedding model to the so-called word2vec format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#bigmodel = api.load(\"word2vec-google-news-300\")\n",
    "# w2vmodel = dict(zip(bigmodel.index2word, bigmodel.syn0))\n",
    "\n",
    "w2vmodel = dict(zip(model.index2word, model.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "0.8468904556455856\n",
      "Recall:\n",
      "0.64384\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', embeddingvectorizer.EmbeddingTfidfVectorizer(w2vmodel, operator='mean')),\n",
    "                    ('svm', \n",
    "                     SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit([e[0] for e in reviews], [e[1] for e in reviews])\n",
    "predictions = mypipe.predict([e[0] for e in test])\n",
    "\n",
    "print('Precision:')\n",
    "print(metrics.precision_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))\n",
    "print('Recall:')\n",
    "print(metrics.recall_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, these embeddings seem to be crap. Let's use the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "bigmodel = api.load(\"word2vec-google-news-300\")\n",
    "w2vmodel = dict(zip(bigmodel.index2word, bigmodel.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "0.9180983252296057\n",
      "Recall:\n",
      "0.67976\n"
     ]
    }
   ],
   "source": [
    "mypipe = Pipeline([('vectorizer', embeddingvectorizer.EmbeddingTfidfVectorizer(w2vmodel, operator='mean')),\n",
    "                    ('svm', \n",
    "                     SGDClassifier(loss='hinge', penalty='l2', tol=1e-4, alpha=1e-6, max_iter=1000, random_state=42))])\n",
    "\n",
    "# Generate BOW representation of word counts\n",
    "mypipe.fit([e[0] for e in reviews], [e[1] for e in reviews])\n",
    "predictions = mypipe.predict([e[0] for e in test])\n",
    "\n",
    "print('Precision:')\n",
    "print(metrics.precision_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))\n",
    "print('Recall:')\n",
    "print(metrics.recall_score([e[1] for e in test],predictions,pos_label='1', labels = ['-1','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft cosine similarity\n",
    "\n",
    "Finally, let's explore soft cosine similarity by re-using our model `model` from above, and our movie reviews `reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n",
    "documents = [e[0].lower().split() for e in reviews[:100]]\n",
    "\n",
    "id2word = Dictionary(documents)\n",
    "bow_corpus = [id2word.doc2bow(document) for document in documents]\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, id2word)  # construct similarity matrix\n",
    "docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''Pulp Fiction may be the single best film ever made, and quite appropriately it is by one of the most \n",
    "creative directors of all time, Quentin Tarantino. This movie is amazing from the beginning definition of pulp to\n",
    "the end credits and boasts one of the best casts ever assembled with the likes of Bruce Willis, Samuel L. Jackson, \n",
    "John Travolta, Uma Thurman, Harvey Keitel, Tim Roth and Christopher Walken. The dialog is surprisingly humorous for\n",
    "this type of film, and I think that's what has made it so successful. Wrongfully denied the many Oscars it was \n",
    "nominated for, Pulp Fiction is by far the best film of the 90s and no Tarantino film has surpassed the quality of\n",
    "this movie (although Kill Bill came close). As far as I'm concerned this is the top film of all-time and definitely \n",
    "deserves a watch if you haven't seen it.\n",
    "'''.lower().split()\n",
    "sims = docsim_index[dictionary.doc2bow(query)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 0.9686975479125977),\n",
       " (50, 0.9650362133979797),\n",
       " (41, 0.9644956588745117),\n",
       " (1, 0.9642953872680664),\n",
       " (31, 0.9641914367675781),\n",
       " (13, 0.9631236791610718),\n",
       " (37, 0.9629360437393188),\n",
       " (17, 0.9595888257026672),\n",
       " (85, 0.9594312310218811),\n",
       " (68, 0.959321916103363)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was blown away by the re-imagined Battlestar Galactica, a show that always kept me guessing and brought me to tears on more than one occasion. A hardened sci-fi fan, I like to think I can pick out the good stuff from the BS, and this was good stuff.<br /><br />As such, when I first heard about the prospect of a prequel series some months ago I got a sick feeling in my gut. I was afraid that the formula that made Battlestar so successful would be reused in Caprica, which wouldn't work at all. BSG's story, of a mournful ragged band of survivors, trapped aboard decaying star ships and guided by prophetic vision and a sequence of pseudo-miracles, was perfectly complimented by extraordinary music and a better cast of actors.<br /><br />Caprica feels different. Where BSG takes place after the fall of a great civilization, Caprica portrays that civilization in it's cold and decadent heyday. The overall vibe I got from Caprica was similar to that of Minority Report, minus excessive and counterproductive theatricality. In true BSG form, Caprica has in it's first few hours of programming already tackled the issues of religious freedom, racism, the morality of playing God and the nature of the human soul.<br /><br />The casting for Caprica is also excellent. Each character is unique and deep, from the obsessive and distant scientist-turned-entrepreneur, to his troubled and willful daughter, each actor and actress throws themselves into their respective roles.<br /><br />Music, which was used so powerfully in BSG, also plays a significant role in Caprica. Battlestar's powerful rolling drums and mournful duduks served it's themes very well. Caprica uses a more orchestral sound, which gives the show it's own feeling quite distinct from either of it's predecessors. <br /><br />The new Caprica is definitely it's own show, pulling from the Battlestar franchise only as much as it needs. I look forward to the full series.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(documents[50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
